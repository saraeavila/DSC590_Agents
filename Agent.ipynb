{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJygm2q2Opa6",
        "outputId": "7d52121c-de68-4bee-ebf4-ee29a99c50c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.79)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Collecting langchain-core>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-1.0.8-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.7-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.0/473.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-1.0.8 langchain-core-1.0.7 langchain-openai-1.0.3 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langgraph langchain langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The API Key should be named \"OPENAI_API_KEY\", while creating the key (https://openrouter.ai/settings/keys)"
      ],
      "metadata": {
        "id": "loRR4XgQl9p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
      ],
      "metadata": {
        "id": "T_jUERbdOy2I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OGh_7lBPI9T",
        "outputId": "7c01ad66-6036-4cae-f864-cead7135cf31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")"
      ],
      "metadata": {
        "id": "umSRm4eSPTF_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class CounterState(TypedDict):\n",
        "    count: int"
      ],
      "metadata": {
        "id": "UKHDJtQXPhKi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increment(state: CounterState) -> dict:\n",
        "    state[\"count\"] += 1\n",
        "    return state"
      ],
      "metadata": {
        "id": "KGI7eE8tPmxf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(CounterState)\n",
        "\n",
        "builder.add_node(\"increment\", increment)\n",
        "\n",
        "# Define the execution order: START -> increment -> END\n",
        "builder.add_edge(START, \"increment\")\n",
        "builder.add_edge(\"increment\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "_9R37jSsPqRL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state: CounterState = {\"count\": 0}\n",
        "\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "print(result)\n",
        "\n",
        "# Output\n",
        "# {'count': 1}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMCgWsGdPtBK",
        "outputId": "dacfa7e3-6538-40dc-e511-16a06b16f73b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'count': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "def should_continue(state: CounterState) -> Literal[\"increment\", END]:\n",
        "\n",
        "    if state[\"count\"] < 10: # keep looping\n",
        "        print(\"count: \", state[\"count\"])\n",
        "        return \"increment\"\n",
        "\n",
        "\n",
        "    return END # stop the graph"
      ],
      "metadata": {
        "id": "GJkyLLbePxIY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(CounterState)\n",
        "\n",
        "builder.add_node(\"increment\", increment)\n",
        "\n",
        "builder.add_edge(START, \"increment\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"increment\",\n",
        "    should_continue,\n",
        "    [\"increment\", END],\n",
        ")\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "result = graph.invoke({\"count\": 0})\n",
        "\n",
        "print(result)\n",
        "\n",
        "# Output\n",
        "# {'count': 3}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfwAmGYoP1Se",
        "outputId": "90c20221-2602-48f5-ff2b-8d4108f516e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count:  1\n",
            "count:  2\n",
            "count:  3\n",
            "count:  4\n",
            "count:  5\n",
            "count:  6\n",
            "count:  7\n",
            "count:  8\n",
            "count:  9\n",
            "{'count': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    user_input: str\n",
        "    response: str"
      ],
      "metadata": {
        "id": "NaZ1IA8PROO4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")"
      ],
      "metadata": {
        "id": "tREjN5UrRPaA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def llm_node(state: AgentState) -> dict:\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "        HumanMessage(content=state[\"user_input\"]),\n",
        "    ]\n",
        "\n",
        "    reply = llm.invoke(messages)\n",
        "\n",
        "    return {\"response\": reply.content}"
      ],
      "metadata": {
        "id": "YST5401VRRs-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"llm\", llm_node)\n",
        "\n",
        "# define the flow: START -> llm -> END\n",
        "builder.add_edge(START, \"llm\")\n",
        "builder.add_edge(\"llm\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "ALUkfCOiRVn6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state: AgentState = {\n",
        "    \"user_input\": \"What is GIL in python?\",\n",
        "    \"response\": \"\",\n",
        "}\n",
        "\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "print(\"User:\", initial_state[\"user_input\"])\n",
        "print(\"Assistant:\", result[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GykLlHBdRiWK",
        "outputId": "4b0c56c3-dd69-488a-ef1f-6b9ed08fd78c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is GIL in python?\n",
            "Assistant: GIL, or Global Interpreter Lock, is a mechanism used in CPython (the standard implementation of Python) to ensure that only one thread executes Python bytecode at a time. This lock is necessary because Python's memory management is not thread-safe; access to Python objects and internal data structures must be synchronized to prevent corruption.\n",
            "\n",
            "### Key Points about GIL:\n",
            "\n",
            "1. **Thread Safety**: The GIL simplifies memory management and ensures thread safety for Python objects, avoiding the complexities of multi-threading issues like race conditions, which can arise without a locking mechanism.\n",
            "\n",
            "2. **Performance Implications**: Due to the GIL, even in multi-threaded Python programs, only one thread can execute Python code at a time. This means that while I/O-bound tasks can benefit from threading (where the GIL can be released during I/O operations), CPU-bound tasks may not see performance improvements with threads, as they cannot execute in parallel on multiple CPU cores.\n",
            "\n",
            "3. **I/O-bound vs CPU-bound**: For I/O-bound operations (such as network requests, file I/O), multi-threading can be beneficial despite the GIL, since threads can release the GIL while waiting for I/O operations to complete. Conversely, for CPU-bound operations (heavy computations), using multiprocessing (which starts separate processes, each with its own Python interpreter and memory space, and thus its own GIL) is usually more effective.\n",
            "\n",
            "4. **Alternative Implementations**: Other Python implementations, such as Jython (Python on the JVM) or IronPython (Python on .NET) do not have a GIL and can utilize multiple threads more effectively. Additionally, libraries like Cython or alternatives like Numba can help with performance in multi-threaded scenarios.\n",
            "\n",
            "### Conclusion:\n",
            "The GIL is a well-known characteristic of CPython that affects multi-threading in Python programs. While it simplifies certain aspects of memory management and can make programming easier by avoiding threading issues, it also imposes limitations on the concurrency and parallelism of CPU-bound applications. For many use cases, understanding how to work with the GIL is crucial for optimizing performance in Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state2: AgentState = {\n",
        "    \"user_input\": \"Can you summarise that in two lines?\",\n",
        "    \"response\": \"\",\n",
        "}\n",
        "result2 = graph.invoke(state2)\n",
        "\n",
        "print(\"\\nTurn 2 - User:\", state2[\"user_input\"])\n",
        "print(\"Turn 2 - Assistant:\", result2[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvy6H7fyY5-X",
        "outputId": "a5e400a8-d557-49f3-fb15-b2ef83e4e6be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Turn 2 - User: Can you summarise that in two lines?\n",
            "Turn 2 - Assistant: Sure! Please provide the text you would like summarized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, Annotated\n",
        "from operator import add\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "class State(TypedDict):\n",
        "    foo: str\n",
        "    bar: Annotated[list[str], add]\n",
        "\n",
        "def node_a(state: State):\n",
        "    # overwrite foo, append \"a\" to bar\n",
        "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
        "\n",
        "def node_b(state: State):\n",
        "    # overwrite foo, append \"b\" to bar\n",
        "    return {\"foo\": \"b\", \"bar\": [\"b\"]}"
      ],
      "metadata": {
        "id": "8thKkKNza46n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(State)\n",
        "builder.add_node(\"node_a\", node_a)\n",
        "builder.add_node(\"node_b\", node_b)\n",
        "\n",
        "builder.add_edge(START, \"node_a\")\n",
        "builder.add_edge(\"node_a\", \"node_b\")\n",
        "builder.add_edge(\"node_b\", END)\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "S5gy2b0lbLfa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "final_state = graph.invoke({\"foo\": \"\", \"bar\": []}, config=config)\n",
        "print(\"Final state:\", final_state)\n",
        "\n",
        "# Output\n",
        "# Final state: {'foo': 'b', 'bar': ['a', 'b']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmLXOoLTbMf5",
        "outputId": "154c5ee0-d0f0-42e4-ed6f-256f706e6281"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final state: {'foo': 'b', 'bar': ['a', 'b']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = list(graph.get_state_history(config))\n",
        "\n",
        "for i, snap in enumerate(history[::-1]):\n",
        "    print(f\"\\nCheckpoint {i}:\")\n",
        "    print(\"  created_at:\", snap.created_at)\n",
        "    print(\"  node:\", snap.metadata)\n",
        "    print(\"  values:\", snap.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELtyVBjxbQZY",
        "outputId": "5bf086e9-5de2-4518-d3cc-67b1b532bea4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint 0:\n",
            "  created_at: 2025-11-20T21:55:24.450980+00:00\n",
            "  node: {'source': 'input', 'step': -1, 'parents': {}}\n",
            "  values: {'bar': []}\n",
            "\n",
            "Checkpoint 1:\n",
            "  created_at: 2025-11-20T21:55:24.454246+00:00\n",
            "  node: {'source': 'loop', 'step': 0, 'parents': {}}\n",
            "  values: {'foo': '', 'bar': []}\n",
            "\n",
            "Checkpoint 2:\n",
            "  created_at: 2025-11-20T21:55:24.456344+00:00\n",
            "  node: {'source': 'loop', 'step': 1, 'parents': {}}\n",
            "  values: {'foo': 'a', 'bar': ['a']}\n",
            "\n",
            "Checkpoint 3:\n",
            "  created_at: 2025-11-20T21:55:24.459949+00:00\n",
            "  node: {'source': 'loop', 'step': 2, 'parents': {}}\n",
            "  values: {'foo': 'b', 'bar': ['a', 'b']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, Annotated\n",
        "from langchain_core.messages import AnyMessage\n",
        "import operator\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]"
      ],
      "metadata": {
        "id": "HPRqRWNBbWuU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "def chat_llm_node(state: MessagesState) -> dict:\n",
        "\n",
        "    # build prompt from system + existing conversation\n",
        "    history = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
        "    history.extend(state[\"messages\"])\n",
        "\n",
        "    reply = llm.invoke(history)\n",
        "\n",
        "    return {\"messages\": [reply]}"
      ],
      "metadata": {
        "id": "3QsB8Jb3bbvj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"chat_llm\", chat_llm_node)\n",
        "builder.add_edge(START, \"chat_llm\")\n",
        "builder.add_edge(\"chat_llm\", END)\n",
        "\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "QPLmmTf_bdUh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n"
      ],
      "metadata": {
        "id": "5wf5Dh1AbgX-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n",
        "\n",
        "# Turn 1\n",
        "state1 = {\"messages\": [HumanMessage(content=\"What is GIL in python?\")]}\n",
        "result1 = graph.invoke(state1, config=config)\n",
        "\n",
        "for m in result1[\"messages\"]:\n",
        "    print(type(m).__name__, \":\", m.content)\n",
        "\n",
        "# Turn 2\n",
        "state2 = {\"messages\": [HumanMessage(content=\"Summarise it in two lines\")],}\n",
        "result2 = graph.invoke(state2, config=config)\n",
        "\n",
        "for m in result2[\"messages\"][-2:]:\n",
        "    print(type(m).__name__, \":\", m.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhgqZdMmbjL1",
        "outputId": "ba38e50d-7b84-4daf-ee11-a3e984498cd5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage : What is GIL in python?\n",
            "AIMessage : GIL stands for Global Interpreter Lock. It is a mechanism used in Python (specifically CPython, the most common implementation of Python) to ensure that only one thread executes Python bytecode at a time. Here are some key points about GIL:\n",
            "\n",
            "1. **Thread Safety**: The GIL makes it easier to manage memory and ensures thread safety with respect to Python's internal data structures. It prevents race conditions when multiple threads interact with Python objects.\n",
            "\n",
            "2. **Concurrency Limitation**: While the GIL simplifies development in multithreaded programs, it can also be a limitation for CPU-bound programs. When CPU-bound tasks are executed in multiple threads, the GIL forces one thread to run at a time, which can lead to suboptimal performance compared to processes that can run truly concurrently.\n",
            "\n",
            "3. **I/O-Bound vs CPU-Bound**: The GIL is less of an issue for I/O-bound programs (those that spend a lot of time waiting for external events, like network responses or disk I/O) because while one thread is waiting, the GIL can be released allowing other threads to run. In contrast, CPU-bound programs benefit less from threading because they are limited by the GIL.\n",
            "\n",
            "4. **Alternatives to Threading**: To fully utilize multicore processors in Python, especially for CPU-bound tasks, developers often use multiprocessing instead of multithreading. The `multiprocessing` module allows the creation of separate processes, each with its own Python interpreter and memory space, effectively bypassing the GIL.\n",
            "\n",
            "5. **Python Implementations**: It's important to note that the GIL is specific to CPython. Other implementations of Python, such as Jython (Python on the Java platform) or IronPython (Python on .NET), may not have a GIL or might handle concurrency differently.\n",
            "\n",
            "In summary, the GIL is a fundamental part of CPython that affects how threads are managed and can impact the performance of concurrent Python programs.\n",
            "HumanMessage : Summarise it in two lines\n",
            "AIMessage : The Global Interpreter Lock (GIL) in Python (CPython) ensures that only one thread executes Python bytecode at a time, simplifying memory management and thread safety. However, it limits the performance of CPU-bound programs by preventing true parallelism across multiple threads, making `multiprocessing` a better choice for utilizing multicore processors.\n"
          ]
        }
      ]
    }
  ]
}